{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35fa80c4",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "![iteso](https://upload.wikimedia.org/wikipedia/en/5/5f/Western_Institute_of_Technology_and_Higher_Education_logo.png)\n",
    "\n",
    "###  InstitutoTecnológico y de Estudios Superiores de Occidente ###\n",
    "###  Maestría Ciencia de Datos  ###\n",
    "###  Investigación, Desarrollo e Innovación 2  ###\n",
    "###  Tarea 7: Aplicación perceptrón multicapa  ###\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "* * *\n",
    "\n",
    "Estudiante: Daniel Nuño <br>\n",
    "Profesor: Fernando Becerra <br>\n",
    "Fecha entrega: Octubre 23, 2021 <br>\n",
    "\n",
    "* * *\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5eb965ab",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f378d4",
   "metadata": {},
   "source": [
    "En la tabla siguiente se muestran los datos de 1000 clientes que solicitaron créditos a un banco dado. La última columna muestra la información de los clientes que cayeron en mora en algún momento del período del crédito. El monto máximo de crédito que puede asignarse son $300,000 y la antigüedad laboral máxima que se toma en cuenta para asignar el crédito es de 15 años (es decir, antigüedades mayores ya no generan más posibilidad de ser aprobado).\n",
    "\n",
    "Se busca una relación entre la información presentada (que se obtiene al contratar el crédito) y la posibilidad de que el cliente caiga en mora en algún momento del plazo.\n",
    "\n",
    "Entrene un perceptrón multicapa para encontrar una relación tomando como entradas el monto solicitado (normalizado), la carga que implica al salario el pago de la mensualidad y la antigüedad laboral al contratar (normalizada).\n",
    "\n",
    "Utilice 70-30 de relación entrenamiento-prueba y calcule el accuracy.\n",
    "\n",
    "El número de neuronas ocultas es a su criterio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f96c2daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entidad</th>\n",
       "      <th>Monto</th>\n",
       "      <th>Mensualidad</th>\n",
       "      <th>Plazo (años)</th>\n",
       "      <th>Tasa anual</th>\n",
       "      <th>Ingreso mensual</th>\n",
       "      <th>Antigüedad laboral (meses)</th>\n",
       "      <th>Mora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sinaloa</td>\n",
       "      <td>299200</td>\n",
       "      <td>6277.86</td>\n",
       "      <td>20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>33911.00</td>\n",
       "      <td>58</td>\n",
       "      <td>SI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Michoacán de Ocampo</td>\n",
       "      <td>281100</td>\n",
       "      <td>9373.58</td>\n",
       "      <td>20</td>\n",
       "      <td>0.40</td>\n",
       "      <td>112783.48</td>\n",
       "      <td>149</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nuevo León</td>\n",
       "      <td>268800</td>\n",
       "      <td>8963.43</td>\n",
       "      <td>20</td>\n",
       "      <td>0.40</td>\n",
       "      <td>33186.96</td>\n",
       "      <td>134</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Guerrero</td>\n",
       "      <td>256600</td>\n",
       "      <td>9106.81</td>\n",
       "      <td>5</td>\n",
       "      <td>0.35</td>\n",
       "      <td>51118.90</td>\n",
       "      <td>77</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yucatán</td>\n",
       "      <td>256500</td>\n",
       "      <td>5381.92</td>\n",
       "      <td>20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>197168.90</td>\n",
       "      <td>5</td>\n",
       "      <td>SI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Entidad   Monto  Mensualidad  Plazo (años)  Tasa anual  \\\n",
       "0              Sinaloa  299200      6277.86            20        0.25   \n",
       "1  Michoacán de Ocampo  281100      9373.58            20        0.40   \n",
       "2           Nuevo León  268800      8963.43            20        0.40   \n",
       "3             Guerrero  256600      9106.81             5        0.35   \n",
       "4              Yucatán  256500      5381.92            20        0.25   \n",
       "\n",
       "   Ingreso mensual  Antigüedad laboral (meses) Mora  \n",
       "0         33911.00                          58   SI  \n",
       "1        112783.48                         149   NO  \n",
       "2         33186.96                         134   NO  \n",
       "3         51118.90                          77   NO  \n",
       "4        197168.90                           5   SI  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def train_one_layer_perceptron(training_data, training_output, L, a, alpha, accuracy, max_epochs):\n",
    "    # INITIALIZE values and functions for one hidden layer\n",
    "    a = a #a = inclinación de la sigmoide tiene que ser mayor a 0\n",
    "    alpha = alpha #alpha learning rate tiene que ser entre 0 y 1\n",
    "    L = L #number of neurons per layer\n",
    "    accuracy = accuracy\n",
    "    error = 1\n",
    "    \n",
    "    x = training_data #Q * N matrix. Q = rows, N = columns\n",
    "    d = training_output #Q * M matrix. Q = rows, M = columns\n",
    "    N = training_data.shape[1] #number of Xs = entries columns\n",
    "    M = training_output.shape[1] #number of Ys = number of columns outputs\n",
    "    Q = training_data.shape[0] #different inputs with known outputs = number of rows\n",
    "\n",
    "    epoch_count = 0\n",
    "    np.random.seed(1)\n",
    "    weights_ItoH = np.random.uniform(-1, 1, (L, N)) #L x N size\n",
    "    weights_HtoO = np.random.uniform(-1, 1, (M, L)) #M x L size\n",
    "\n",
    "    while error > accuracy and (epoch_count < max_epochs):\n",
    "        for row in range(Q):\n",
    "            #feedforward\n",
    "            net_h = weights_ItoH @ np.reshape(x[row,:], (N,1))\n",
    "            y_h = logistic(net_h,a)\n",
    "            net_o = weights_HtoO @ np.reshape(y_h, (L,1))\n",
    "            y_o = logistic(net_o,a)\n",
    "\n",
    "            #backforward\n",
    "            delta_o = np.reshape( d[row,:] - np.reshape(y_o, (1,M)), (M,1)) * (y_o * (1 - y_o))\n",
    "            delta_h = (y_o * (1 - y_o)) * (np.transpose(weights_HtoO) @ delta_o)\n",
    "            weights_HtoO += alpha * (delta_o * np.transpose(y_h))\n",
    "            weights_ItoH += alpha * (delta_h * training_data[row,:])\n",
    "            \n",
    "            #error\n",
    "            error = np.linalg.norm(delta_o)\n",
    "        epoch_count +=1\n",
    "    return (weights_HtoO, weights_ItoH, error, epoch_count)\n",
    "\n",
    "def logistic(x,a):\n",
    "    return 1/(1 + np.exp(-a*x))\n",
    "\n",
    "def logistic_deriv(x,a):\n",
    "    return a*logistic(x,a) * (1 - logistic(x,a))\n",
    "\n",
    "def evaluate_one_layer_perceptron(input_matrix, weights_HtoO, weights_ItoH, a):\n",
    "    # INITIALIZE values\n",
    "    x = input_matrix\n",
    "    weights_HtoO = weights_HtoO\n",
    "    weights_ItoH = weights_ItoH\n",
    "    a = a\n",
    "    N = x.shape[1] #number of Xs = entries columns\n",
    "    Q = x.shape[0] #different inputs with known outputs = number of rows\n",
    "\n",
    "    output_matrix = []\n",
    "    output_row = []\n",
    "    y_matrix = []\n",
    "\n",
    "    #cicle through rows to evaluate according to weights\n",
    "    for row in range(Q):\n",
    "        net_h = weights_ItoH @ np.reshape(x[row,:], (N,1))\n",
    "        y_h = logistic(net_h,a)\n",
    "        net_o = weights_HtoO @ np.reshape(y_h, (L,1))\n",
    "        y_o = logistic(net_o,a)\n",
    "        # from y's hidden to outputs define if is 0 or 1 and append to rows outputs\n",
    "        for y in y_o:\n",
    "            if y > 0.5:\n",
    "                output_row.append(1)\n",
    "            else:\n",
    "                output_row.append(0)\n",
    "\n",
    "        output_matrix.append(output_row) #append row outputs in a matrix\n",
    "        output_row = [] #clear to be used again\n",
    "        y_matrix.append(y_o) #append return\n",
    "\n",
    "    output_matrix = np.asarray(output_matrix, dtype=np.float64)\n",
    "    y_matrix = np.asarray(y_matrix, dtype=np.float64)\n",
    "\n",
    "    return np.block([x, output_matrix]), output_matrix, y_matrix\n",
    "\n",
    "\n",
    "data = pd.read_excel('C:/Users/nuno/OneDrive - ITESO/Ciencia de Datos'\n",
    "                     '/IDI II/PercMultAplicado_tarea7.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe416a3",
   "metadata": {},
   "source": [
    "La table contiene la **Entidad**, **Monto**, **Mensualidad**, **Plazo (años)**, **Tasa anual**, **Ingreso mensual**, **Antigüedad laboral (meses)** que se pueden considerar variables de entrada que determinan **Mora**.\n",
    "Como discutido en clase tenemos que preprocesar los datos.\n",
    "\n",
    "Primero Entidad, aunque puede ser aportar para saber si el crédito sera moratorio no la podemos usar porque no es un número. Convertirla en número consecutivo no es buena idea por que implicaría que una Entidad tiene mayor peso que otra sin razón aparente. Tendríamos que utilizar alguna técnica estadística para convertirlo a número.\n",
    "\n",
    "Segundo, mensualidad es dependiente y resultado del Plazo (años), Tasa anual y Monto. Plaza (años) y Tasa anual la podemos descartar.\n",
    "\n",
    "Tercero, sabemos que Monto y Antigüedad laboral (meses) están limitados a $300,000 y 180 meses (15 años x 12 meses) respectivamente. Normalizamos ambos para que los valores sean (0 a 1).\n",
    "\n",
    "Normalizamos con una regla de 3, lo cual nos da resultados como si fuera una distribución lineal, aunque no es necesariamente la mejor aproximación ya que los datos podrían no estar distribuidos linealmente. Podrían aproximarse más a una exponencial, logística, normal, etc.\n",
    "\n",
    "Cuarto, carga al salario se define como la Mensualidad / Ingreso mensual. Nos ayuda porque nos da valores entre 0 y 1 y, contiene información de la mensualidad y el ingreso mensual en términos del tamaño de la deuda para su capacidad.\n",
    "\n",
    "Quinto, transformar la columna Mora a 0 y 1. SI equivale a 1 y NO es igual a 0.\n",
    "\n",
    "La final tenemos solo las siguientes columnas como entradas y salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a03e858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Monto</th>\n",
       "      <th>Antigüedad laboral (meses)</th>\n",
       "      <th>Mora</th>\n",
       "      <th>carga_al_salario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.997333</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>1</td>\n",
       "      <td>0.185128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.937000</td>\n",
       "      <td>0.827778</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>0</td>\n",
       "      <td>0.270089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.855333</td>\n",
       "      <td>0.427778</td>\n",
       "      <td>0</td>\n",
       "      <td>0.178150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Monto  Antigüedad laboral (meses)  Mora  carga_al_salario\n",
       "0  0.997333                    0.322222     1          0.185128\n",
       "1  0.937000                    0.827778     0          0.083111\n",
       "2  0.896000                    0.744444     0          0.270089\n",
       "3  0.855333                    0.427778     0          0.178150\n",
       "4  0.855000                    0.027778     1          0.027296"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['carga_al_salario'] = data.Mensualidad / data['Ingreso mensual']\n",
    "data.Monto = data.Monto / 300000\n",
    "data['Antigüedad laboral (meses)'] = data['Antigüedad laboral (meses)'] / 180\n",
    "data['Mora'] = data['Mora'].replace('SI', 1)\n",
    "data['Mora'] = data['Mora'].replace('NO', 0)\n",
    "data = data.drop(['Entidad',\n",
    "                  'Plazo (años)',\n",
    "                  'Tasa anual',\n",
    "                  'Mensualidad',\n",
    "                  'Ingreso mensual'], axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3520dc",
   "metadata": {},
   "source": [
    "Ahora definimos entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30b74e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data.sample(frac=0.7)\n",
    "test_data = data.drop(training_data.index)\n",
    "\n",
    "training_output = training_data[['Mora']]\n",
    "training_output = np.asarray(training_output, dtype=np.float64)\n",
    "training_data = training_data[['Monto', 'Antigüedad laboral (meses)',\n",
    "                               'carga_al_salario']]\n",
    "training_data = np.asarray(training_data,dtype=np.float64)\n",
    "\n",
    "test_output = test_data[['Mora']]\n",
    "test_output = np.asarray(test_output, dtype=np.float64)\n",
    "test_data = test_data[['Monto', 'Antigüedad laboral (meses)',\n",
    "                       'carga_al_salario']]\n",
    "test_data = np.asarray(test_data, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf77a07",
   "metadata": {},
   "source": [
    "Lo que quiero hacer primero es definir un límite inferior que indique el resultado que debo obtener con una estimación y modelo más sencillo o conocido o implementado. En este caso lo más sencillo seria definir dicho limite como lo más probable que suceda y darles ese valor a todos los resultados estimados; es decir que tan probable es que el crédito NO entre en mora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6af0f0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7466666666666667"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_limit = np.sum(test_output == 0) / len(test_output)\n",
    "base_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2567d",
   "metadata": {},
   "source": [
    "Ya que tenemos nuestro límite de 74.3% (o 84.1% para todo el data set), queremos que nuestro modelo sea más preciso. De otra manera podríamos definir que todos los nuevos créditos no caerán en mora sin la necesidad de un perceptrón.\n",
    "\n",
    "Primero voy a intentar una sencilla combinación en cuanto **a = 1**, **neuronas = 6** y **alpha = 0.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2460fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "alpha = 0.1\n",
    "L = 6\n",
    "accuracy = 0.0001\n",
    "max_epochs = 100000\n",
    "\n",
    "(weights_HtoO,\n",
    " weights_ItoH,\n",
    " error,\n",
    " epoch_count) = train_one_layer_perceptron(training_data,\n",
    "                                           training_output,\n",
    "                                           L,\n",
    "                                           a,\n",
    "                                           alpha,\n",
    "                                           accuracy,\n",
    "                                           max_epochs)\n",
    "\n",
    "(x_and_y,\n",
    " transformed_y,\n",
    " direct_y) = evaluate_one_layer_perceptron(test_data,\n",
    "                                         weights_HtoO,\n",
    "                                         weights_ItoH,\n",
    "                                         a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14bc4b",
   "metadata": {},
   "source": [
    "Habiendo entrenado y evaluado, sigue calcular la precisión.\n",
    "\n",
    "*direct_y es el vector de salida evaluado con el perceptrón, transformed_y es la transformación a 0 ó 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84e0b0c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9966666666666667"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_accuracy = test_output == transformed_y\n",
    "accuracy_validation = vector_accuracy.sum() / len(test_data)\n",
    "accuracy_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c09f1",
   "metadata": {},
   "source": [
    "También queremos saber cuántas veces el perceptrón es indeciso, porque no está muy seguro que el resultado sea 1 o que sea 0 considerando los resultados como probabilidad $p$ y $(1-p)$. Para eso (arbitrariamente) definimos el intervalo 0.1 a 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fbd22c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3466666666666667"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_sure = np.sum(direct_y[direct_y > 0.1] < 0.9) / len(test_data)\n",
    "less_sure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c979843",
   "metadata": {},
   "source": [
    "El accuracy es bastante bueno cerca de 100%. En cuanto los resultados que no está muy seguro obtuvimos 20%.\n",
    "\n",
    "Sabemos que queremos que nuestros resultados sean menos ambiguos, sin comprometer la precisión. Entonces ahora intentare con diferentes neuronas = 20, alpha = 0.1 y a = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d4ee7bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 10\n",
    "alpha = 0.2\n",
    "L = 12\n",
    "accuracy = 0.001\n",
    "max_epochs = 100000\n",
    "\n",
    "(weights_HtoO,\n",
    " weights_ItoH,\n",
    " error,\n",
    " epoch_count) = train_one_layer_perceptron(training_data,\n",
    "                                           training_output,\n",
    "                                           L,\n",
    "                                           a,\n",
    "                                           alpha,\n",
    "                                           accuracy,\n",
    "                                           max_epochs)\n",
    "\n",
    "(x_and_y,\n",
    " transformed_y,\n",
    " direct_y) = evaluate_one_layer_perceptron(test_data,\n",
    "                                         weights_HtoO,\n",
    "                                         weights_ItoH,\n",
    "                                         a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0d767ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7466666666666667"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_accuracy = test_output == transformed_y\n",
    "accuracy_validation = vector_accuracy.sum() / len(test_data)\n",
    "accuracy_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b0fcfb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_sure = np.sum(direct_y[direct_y > 0.1] < 0.9) / len(test_data)\n",
    "less_sure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63208a29",
   "metadata": {},
   "source": [
    "El accuracy 75% y los resultados que no está muy seguro 2%. \n",
    "Empeoro en el accuracy pero mejoro substancialmente en la confiabilidad.\n",
    "\n",
    "Definitivamente una alta a y más neuronas empeoran el modelo porque está empujando todos los resultados a 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
